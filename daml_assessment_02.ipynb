{"cells":[{"cell_type":"markdown","metadata":{"id":"ZNWbaIDYFbK2"},"source":["---\n","# Data Analysis and Machine Learning\n","---\n","## Assessment 02\n","\n","The University of York campus lake on the Heslington West campus is home to a lot of different lake birds; in fact, the University has the second highest '*duck density*' in the UK! All University of York students are able to recognise the ducks, geese, and swans that they see around the lake - but is a computer able to, or is it what separates us from machines?\n","\n","Your dataset is packaged as a .zip archive (which you will need to download and unpack) and contains colour (RGB) images (***X***) of ducks, geese, and swans (*y*). Inside the .zip archive (`lake_bird_images.zip`), there are two subdirectories: `train` and `test`, containing the training and testing datasets, respectively. Inside each of these subdirectories are three further subdirectories: `duck`, `goose`, and `swan`. There are 498 images of ducks, 981 images of geese, and 335 images of swans inside `train` (1814 images in total), and there are 218 images of ducks, 405 images of geese, and 160 images of swans inside `test` (783 images in total).\n","\n","Your task is to build one deep machine-learning model:\n","- a **multiclass classification model for predicting whether the image is a duck, a goose, or a swan**. You are only allowed to evaluate your model performance on the test dataset (`test`) once; all model (hyperparameter) tuning should be carried out using only the training dataset (`train`) and a validation set derived from it.\n","\n","This assessment also has a written/report-style component which you can complete inside this notebook by adding additional text blocks if necessary. Once you have built your deep machine-learning model, you should:\n","- evaluate its performance, **producing at least three figures that illustrate the performance of the model**, and **write an analysis of each figure that outlines what the figure is showing and what it tells you about the performance of your model**. You are not limited to only three figures - you can produce more figures if they are useful in illustrating a point - although only three figures and accompanying analyses will count towards your grade on the assessment (these will be the highest-graded three that you present). The figures and accompanying analuses can focus on the training/validation performance, the testing performance, or - ideally - a mixture of the two;\n","- answer the question: **what limits the performance of the model?** Up to three proposed explanations will count towards your grade on the assessment.\n","\n","You are limited to 2500 words for your written/report-style contribution, but this is **not** a guideline - it is likely much, much more than what you'll need, and contributions of this length are not expected.\n","\n","All code should generally be commented where appropriate as good practice dictates. When you have finished, use the option on the File menu to download this notebook as in .ipynb format and upload it to the submission point on the VLE.\n","\n","## Tips:\n","\n","- If you cannot get code to work, comment it out and write comments about what you are trying to do and how it fails.\n","- Consider running your code locally on your computer or on a University-managed computer rather than Google Colab to avoid uploading the duck/goose/swan dataset to your Google Drive; the dataset contains around 2500 images and will not only be slow to upload but also slow to access for your deep machine-learning model. Your code will run much, much quicker if you run it offline!\n","- Don't expect the kind of accuracy that you were able to acheive in the last assessment (DAML Assessment 01); this is a much, much more challenging problem! Think, instead, about the baseline accuracy that you might expect for a multiclass classification task like this.\n","- Familiarise yourself with the new TensorFlow notebooks on deep neural networks (DNNs) and deep convolutional neural networks (CNNs) before you attempt the task."]},{"cell_type":"markdown","metadata":{"id":"o60v467SesrH"},"source":["Before you start, click the &#x25B8; icon below to allow colab to access the data files in your drive (not necessary if you plan to work offline)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zeiwxmjkFYoD"},"outputs":[],"source":["from google.colab import drive; drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"rAkfVjZIVdgZ"},"source":["... and click the &#x25B8; icon below to import the `numpy`, `matplotlib.pyplot` libraries:"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"lwlOvM-bVevy"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","metadata":{"id":"aGnoB_5bPQjX"},"source":["You can click the &#x25B8; icon below to install TensorFlow if the environment/computer you're working on doesn't have TensorFlow installed already:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xwaw1uWAPe3S"},"outputs":[],"source":["! pip install tensorflow"]},{"cell_type":"markdown","metadata":{"id":"bIRUOagpPg-V"},"source":["...and click the &#x25B8; icon below to import the `tensorflow` and `tensorflow.keras` libraries:"]},{"cell_type":"code","execution_count":33,"metadata":{"id":"opa5966WP1YV"},"outputs":[],"source":["import tensorflow as tf\n","import tensorflow.keras as keras"]},{"cell_type":"markdown","metadata":{"id":"9wTOJ4fGSjNb"},"source":["### Task 01\n","\n","Build and fit a deep machine-learning model to classify the images of ducks, geese, and swans in `lake_bird_images.zip`. Evaluate your multiclass classification model using the accuracy, and optimise the hyperparameters of your multiclass classification model to obtain the best performance possible on unseen data using the images in `train`. When you are satisified - **and only once in the notebook** - evaluate and/or produce predictions for the images in `test`.\n","\n","You are recommended to use a deep convolutional neural network (CNN) to solve the task. Show evidence that you have:\n","\n","- experimented with the structure and number of the layers (*e.g.* `layers.Conv2D`, `layers.MaxPooling2D`) in your CNN;\n","- experimented with the addition of other kinds of layers (*e.g.* for data augmentation, and/or regularisation \\[`layers.Dropout`, `layers.BatchNormalzation`\\]);\n","- evaluated your chosen multiclass classification model on held-out data."]},{"cell_type":"code","execution_count":55,"metadata":{"id":"Z6JaxJbFUzl2"},"outputs":[],"source":["## TODO:\n","# build and fit a deep machine-learning model to classify the images of ducks,\n","# geese, and swans in `lake_bird_images.zip`\n","\n","import os\n","import shutil\n","\n","# Path to the original data folder\n","source_dir = 'lake_bird_images/train'\n","\n","# Path to the training and validation datasets\n","train_dir = 'lake_bird_images/train_dataset'\n","val_dir = 'lake_bird_images/val_dataset'\n","\n","# Determine the proportion of training and validation sets (80% for training)\n","train_ratio = 0.8\n","\n","# Traverse the subfolders of each category (duck, goose and swan)\n","for category in ['duck', \n","                 'goose', \n","                 'swan']:\n","    # Get a list of all image files under this category\n","    category_dir = os.path.join(source_dir, category)\n","    image_files = os.listdir(category_dir)\n","    num_images = len(image_files)\n","\n","    # Determine cut points to split data into training and validation sets\n","    split_point = int(num_images * train_ratio)\n","\n","    # Copy images to the training set folder\n","    for image_file in image_files[:split_point]:\n","        source_path = os.path.join(category_dir, image_file)\n","        target_path = os.path.join(train_dir, category, image_file)\n","        shutil.copyfile(source_path, target_path)\n","\n","    # Copy images to the validation set folder\n","    for image_file in image_files[split_point:]:\n","        source_path = os.path.join(category_dir, image_file)\n","        target_path = os.path.join(val_dir, category, image_file)\n","        shutil.copyfile(source_path, target_path)"]},{"cell_type":"code","execution_count":63,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":[" 'duck' training: 399\n"," 'duck' validation: 100\n"," 'goose' training: 784\n"," 'goose' validation: 197\n"," 'swan' training: 268\n"," 'swan' validation: 67\n"]}],"source":["categories = ['duck', 'goose', 'swan']\n","data_root = 'lake_bird_images'  # 数据根目录\n","\n","# Dictionary storing the number of images in the training set\n","num_images_train = {}  \n","# Dictionary storing the number of images in the validation set\n","num_images_val = {} \n","\n","for category in categories:\n","    train_folder = os.path.join(data_root, 'train_dataset', category)\n","    val_folder = os.path.join(data_root, 'val_dataset', category)\n","    \n","    num_train = len(os.listdir(train_folder))\n","    num_val = len(os.listdir(val_folder))\n","    \n","    num_images_train[category] = num_train\n","    num_images_val[category] = num_val\n","\n","# print number of each category of training and validation set\n","for category in categories:\n","    print(f\" '{category}' training: {num_images_train[category]}\")\n","    print(f\" '{category}' validation: {num_images_val[category]}\")\n"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 2079 images belonging to 2 classes.\n","Found 518 images belonging to 2 classes.\n","--\n","Total training images: <keras.src.preprocessing.image.DirectoryIterator object at 0x2856c8850>\n"]},{"ename":"NameError","evalue":"name 'total_val' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32m/Users/seanwang/Desktop/Python/daml_2/daml_2/daml_assessment_02.ipynb Cell 12\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/seanwang/Desktop/Python/daml_2/daml_2/daml_assessment_02.ipynb#X24sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m--\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/seanwang/Desktop/Python/daml_2/daml_2/daml_assessment_02.ipynb#X24sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTotal training images:\u001b[39m\u001b[39m\"\u001b[39m, train_generator)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/seanwang/Desktop/Python/daml_2/daml_2/daml_assessment_02.ipynb#X24sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTotal validation images:\u001b[39m\u001b[39m\"\u001b[39m, total_val)\n","\u001b[0;31mNameError\u001b[0m: name 'total_val' is not defined"]}],"source":["# 数据预处理和增强\n","DataGenerator = ImageDataGenerator(\n","    rescale=1./255,\n","    shear_range=0.2,\n","    zoom_range=0.2,\n","    horizontal_flip=True,\n","    validation_split=0.2\n",")\n","\n","# 加载数据\n","train_generator = DataGenerator.flow_from_directory(\n","    dataset,\n","    target_size=image_size,\n","    batch_size=32,\n","    class_mode='categorical',\n","    subset='training'\n",")\n","validation_generator = DataGenerator.flow_from_directory(\n","    dataset,\n","    target_size=image_size,\n","    batch_size=32,\n","    class_mode='categorical',\n","    subset='validation'\n",")\n","\n","print(\"--\")\n","print(\"Total training images:\", train_generator)\n","print(\"Total validation images:\", total_val)"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":202,"status":"ok","timestamp":1702640234598,"user":{"displayName":"Conor Rankine","userId":"06810270915558724800"},"user_tz":0},"id":"y_oEfl1iQffI"},"outputs":[],"source":["## TODO:\n","# plot a figure that illustrates the performance of your deep machine-learning\n","# model, eg. a confusion matrix, a hyperparameter optimisation curve, a training/\n","# validation loss curve, a learning curve, etc."]},{"cell_type":"markdown","metadata":{"id":"i6B_q1RDQzda"},"source":["**TODO:** Write an analysis of the figure above here; what does the figure show, and what does the figure indicate about the performance of your model?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kCzrAO8HRDbl"},"outputs":[],"source":["## TODO:\n","# plot a figure that illustrates the performance of your deep machine-learning\n","# model, eg. a confusion matrix, a hyperparameter optimisation curve, a training/\n","# validation loss curve, a learning curve, etc."]},{"cell_type":"markdown","metadata":{"id":"SQctw54nRFlD"},"source":["**TODO:** Write an analysis of the figure above here; what does the figure show, and what does the figure indicate about the performance of your model?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YZo6FYQDREOI"},"outputs":[],"source":["## TODO:\n","# plot a figure that illustrates the performance of your deep machine-learning\n","# model, eg. a confusion matrix, a hyperparameter optimisation curve, a training/\n","# validation loss curve, a learning curve, etc."]},{"cell_type":"markdown","metadata":{"id":"CMcEKLa0RGW7"},"source":["**TODO:** Write an analysis of the figure above here; what does the figure show, and what does the figure indicate about the performance of your model?"]},{"cell_type":"markdown","metadata":{"id":"UTmOKJVeRKXO"},"source":["**TODO:** What limits the performance of your model? Discuss here, giving up to three possible explanations and (if useful) referring back to your figures."]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"}},"nbformat":4,"nbformat_minor":0}
